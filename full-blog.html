<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training Fair Models That Affect Their Data</title>
    
    <!-- MathJax for LaTeX rendering -->
    <script>
        window.MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']],
                displayMath: [['$$', '$$'], ['\\[', '\\]']],
                processEscapes: true
            },
            options: {
                skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre']
            }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <script>
        // Fix paths for GitHub Pages
        document.addEventListener('DOMContentLoaded', function() {
            // Detect if we're on GitHub Pages
            const isGitHubPages = window.location.hostname.includes('github.io');
            if (isGitHubPages) {
                const baseUrl = '/personal_website/';
                // Add back link
                const header = document.querySelector('h1');
                if (header) {
                    const homeLink = document.createElement('div');
                    homeLink.style.marginBottom = '20px';
                    homeLink.innerHTML = '<p><a href="' + baseUrl + '">‚Üê Back to Home</a></p>';
                    header.parentNode.insertBefore(homeLink, header);
                }
                
                // Fix all image paths
                document.querySelectorAll('img').forEach(img => {
                    const src = img.getAttribute('src');
                    if (src && src.startsWith('static/')) {
                        img.src = baseUrl + src;
                    }
                });
            }
        });
    </script>
    
    <style>
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .image-container {
            text-align: center;
            margin: 20px 0;
        }
        
        .image-container img {
            max-width: 100%;
            height: auto;
        }
        
        .image-row {
            display: flex;
            justify-content: center;
            gap: 10px;
            margin: 20px 0;
            flex-wrap: wrap;
        }
        
        .image-row img {
            max-width: 300px;
            height: auto;
        }
        
        .image-caption {
            font-style: italic;
            margin-top: 10px;
            font-size: 0.9em;
        }
        
        /* Reference styling */
        sup {
            vertical-align: super;
            font-size: smaller;
        }
        
        .citation {
            color: #0066cc;
            text-decoration: none;
            cursor: pointer;
        }
        
        .citation:hover {
            text-decoration: underline;
        }
        
        .references {
            border-top: 1px solid #ddd;
            margin-top: 40px;
            padding-top: 20px;
        }
        
        .reference-item {
            margin-bottom: 15px;
            padding-left: 30px;
            text-indent: -30px;
        }
        
        h1, h2, h3, h4 {
            color: #222;
        }
    </style>
</head>
<body>
    <article>
        <h1>Training Fair Models That Affect Their Data</h1>
        
        <p>The way models classically work is you have training data, you train your model on said data, then you deploy the model into the real world. The assumption is whatever data you use to train your data will be an accurate representation of what the model will see in the real world (i.e., identically distributed). What happens if this isn't the case? Worse, what happens if the action of deploying the model is explicitly what causes the real world to be different from its training data? How do we get around this? How can we train our models to be more efficient in these situations?</p>
        
        <p>This problem becomes even more critical when we consider fairness. Many models are trained to perform well on average cases, which can lead them to ignore or underperform on minority groups within the data. These biases don't just affect model performance; they can be actively reinforced when the model's decisions influence future data. For example, a loan approval model that performs worse for minority groups might reject more applications from these groups unjustly, leading to worse financial outcomes and reinforcing the very disparities present in the training data.</p>
        
        <p>My current research looks to solve this problem. Here's the journey I'm going to take you on,</p>
        
        <ol>
            <li>Performative Prediction
                <ul>
                    <li>When models change their data</li>
                </ul>
            </li>
            <li>Distributionally Robust Optimization
                <ul>
                    <li>Making models robust to changes</li>
                </ul>
            </li>
            <li>Combining the two
                <ul>
                    <li>Training fair models that affect their data</li>
                </ul>
            </li>
        </ol>
        
        <h3>1. Performative Prediction</h3>
        <p>Okay first, what is performative prediction. As we are all aware, machine learning (ML) models are being used more widely than ever for decisions in almost all areas of life. Whether it's in analyzing medical images<sup><a id="cite-ref-1" href="#ref-1" class="citation">[1]</a></sup>, fraud-detection<sup><a id="cite-ref-2" href="#ref-2" class="citation">[2]</a></sup>, or credit risk<sup><a id="cite-ref-2-2" href="#ref-2" class="citation">[2]</a></sup>, machine learning is taking more and more precedence in our lives. As these models start making decisions on increasingly more important data, and larger institutions start using them, we face the very real problem of the models making decisions that influence the outcome they are trying to predict. This is called <em>performative prediction</em>.</p>
        
        <p>As a dummy example to allow us to build intuition, lets look at binary classification. Let's assume we are analyzing the following data,</p>
        <div class="image-container">
            <img src="static/plots/circle_classification_without_groups.png" alt="Binary classification data">
            <p class="image-caption">Figure 1: Binary classification data showing two classes (spam and non-spam) represented by red and blue circles.</p>
        </div>
        
        <p>Here we are looking to separate the blue circles from the red circles. In the real world this might represent spam detection, where the red circles are spam mail and the blue circles are not. As an initial guess we might first start with a model estimate as so,</p>
        
        <div class="image-container">
            <img src="static/plots/binary_classification_with_initial_guess.png" alt="Binary classification data with initial guess">
            <p class="image-caption">Figure 2: Initial classification boundary attempting to separate the two classes.</p>
        </div>
        
        <p>Clearly this is not the best guess. To see how we can change our model, we can first evaluate the (log) loss on this current guess, then try performing a small rotation on it clockwise and counterclockwise to see if the model's loss goes up or down (gradient descent) and follow the direction that decreases the loss. Let's see it in this example.</p>
        
        <div class="image-container">
            <img src="static/plots/binary_class_loss_pert_gain.png" alt="Binary classification data with initial guess">
            <p class="image-caption">Figure 3: Loss evaluation showing how rotating the decision boundary counterclockwise decreases the loss function.</p>
        </div>
        
        <p>Here we notice the loss decreases if we rotate our guess counterclockwise, and so we follow this and repeat the process until we have our best model as shown below.</p>
        
        <div class="image-container">
            <img src="static/plots/binary_classification_with_best_guess_and_loss.png" alt="Binary classification data with initial guess">
            <p class="image-caption">Figure 4: Optimized classification boundary after gradient descent, showing the minimized loss function.</p>
        </div>
        
        <p>Great! Up until this point we have done <em>classical machine learning</em>. Nothing so far has anything to do with models affecting their data or performative prediction. So let's consider that possibility. First notice that the data is actually made up of two distinct groups:</p>
        <div class="image-row">
            <img src="static/plots/binary_classification_with_group_A_only.png" alt="Binary classification data with group A only">
            <img src="static/plots/binary_classification_with_group_B_only.png" alt="Binary classification data with group B only">
        </div>
        <p class="image-caption">Figure 5: The dataset split into two distinct groups - Group A (left) and Group B (right), which could represent different demographic populations.</p>
    </article>
</body>
</html> 